import argparse
import ast
from dataclasses import dataclass, field
import datetime
import json
from k8s_service import K8sService, HelmCommandFailed
import logging
import os
import random
import re
import string
import sys
from tabulate import tabulate
from typing import List, Optional, Tuple

logging.basicConfig(format='%(asctime)s [%(levelname)s] %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

VALID_SOURCE_VERSIONS = ["ES_1.5", "ES_2.4", "ES_5.6", "ES_6.8", "ES_7.10"]
VALID_TARGET_VERSIONS = ["OS_1.3", "OS_2.19", "OS_3.1"]
MA_RELEASE_NAME = "ma"


# Data classes to represent test output generated by python e2e tests
@dataclass
class TestEntry:
    name: str
    description: str
    result: str
    duration: float
    error: Optional[str] = None


@dataclass
class TestSummary:
    passed: int
    failed: int
    source_version: str
    target_version: str


@dataclass
class TestReport:
    summary: TestSummary
    tests: List[TestEntry] = field(default_factory=list)


class TestsFailed(Exception):
    pass


class TestRunner:

    def __init__(self, k8s_service: K8sService, unique_id: str, test_ids: List[str], ma_chart_path: str,
                 combinations: List[Tuple[str, str]]) -> None:
        self.k8s_service = k8s_service
        self.unique_id = unique_id
        self.test_ids = test_ids
        self.ma_chart_path = ma_chart_path
        self.combinations = combinations

    def _print_test_stats(self, report: TestReport) -> None:
        for test in report.tests:
            print(f"{test.name}:")
            print(f"  - result: {test.result}")
            print(f"  - duration: {test.duration:.5f} seconds")
            if test.error:
                print(f"  - error: {test.error}")
            print()

    def _print_summary_table(self, reports: List[TestReport]) -> None:
        all_test_names = sorted({test.name for report in reports for test in report.tests})

        # Build the test matrix rows
        matrix_rows = []
        for report in reports:
            version_label = f"{report.summary.source_version} -> {report.summary.target_version}"
            row = [version_label]
            test_results = {test.name: "✓" if test.result == "passed" else "X" for test in report.tests}
            for name in all_test_names:
                row.append(test_results.get(name, "N/A"))
            matrix_rows.append(row)

        # Build test description rows
        test_descriptions = {}
        for report in reports:
            for test in report.tests:
                test_descriptions.setdefault(test.name, test.description)

        # Print Test Matrix
        headers = ["Version"] + [name[:8] for name in all_test_names]
        print("\nTest Matrix:")
        print(tabulate(matrix_rows, headers=headers, tablefmt="fancy_grid"))

        # Print Test Case Information
        description_table = [[name, test_descriptions[name]] for name in all_test_names]
        print("\nTest Case Information:")
        print(tabulate(description_table, headers=["Test Name", "Description"], tablefmt="fancy_grid"))

        # Print Test Stats
        print("\nTest Stats:")
        for report in reports:
            print(f"===== {report.summary.source_version} -> {report.summary.target_version} =====")
            self._print_test_stats(report)

    def _parse_test_report(self, data: dict) -> TestReport:
        tests = [TestEntry(**test) for test in data.get("tests", [])]
        summary = TestSummary(**data.get("summary"))
        return TestReport(tests=tests, summary=summary)

    def write_report_to_file(self, base_dir: str, report_data: dict, source_version: str, target_version: str):
        dir_normal = base_dir.rstrip("/")
        source_version_normal = source_version.lower().replace("_", "-").replace(".", "-")
        target_version_normal = target_version.lower().replace("_", "-").replace(".", "-")
        file_name = (f"{dir_normal}/test-report-{source_version_normal}-to-{target_version_normal}-"
                     f"{self.unique_id}.json")
        with open(file_name, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

    def collect_reports_and_print_summary(self, reports_dir: str):
        reports = []

        # Iterate over files in the directory
        for file_name in os.listdir(reports_dir):
            if file_name.endswith(".json"):
                file_path = os.path.join(reports_dir, file_name)
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        test_data = json.load(f)
                    test_report = self._parse_test_report(test_data)
                    reports.append(test_report)
                except (OSError, json.JSONDecodeError) as e:
                    print(f"⚠️ Skipping {file_name}, error: {e}")

        self._print_summary_table(reports=reports)

    def run_tests(self, source_version: str, target_version: str, keep_workflows: bool = False,
                  reuse_clusters: bool = False, test_reports_dir: str = None) -> TestReport:
        """Runs pytest tests."""
        logger.info(f"Executing migration test cases with pytest and test ID filters: {self.test_ids}")
        command_list = [
            "pipenv",
            "run",
            "pytest",
            "/root/lib/integ_test/integ_test/ma_workflow_test.py",
            f"--unique_id={self.unique_id}",
            f"--source_version={source_version}",
            f"--target_version={target_version}"
        ]
        if self.test_ids:
            command_list.append(f"--test_ids={','.join(self.test_ids)}")
        if keep_workflows:
            command_list.append("--keep_workflows")
        if reuse_clusters:
            command_list.append("--reuse_clusters")
        command_list.append("-s")
        self.k8s_service.exec_migration_console_cmd(command_list=command_list)
        output_file_path = f"/root/lib/integ_test/results/{self.unique_id}/test_report.json"
        logger.info(f"Retrieving test report at {output_file_path}")
        cmd_response = self.k8s_service.exec_migration_console_cmd(command_list=["cat", output_file_path],
                                                                   unbuffered=False)
        test_data = ast.literal_eval(cmd_response)
        logger.debug(f"Received the following test data: {test_data}")
        if test_reports_dir:
            self.write_report_to_file(base_dir=test_reports_dir, report_data=test_data, source_version=source_version,
                                      target_version=target_version)
        test_report = self._parse_test_report(test_data)
        print(f"Test cases passed: {test_report.summary.passed}")
        print(f"Test cases failed: {test_report.summary.failed}")
        return test_report

    def cleanup_clusters(self) -> None:
        pattern = re.compile(r"^(source|target)-(opensearch|elasticsearch)")
        for install in self.k8s_service.get_helm_installations():
            if pattern.match(install):
                self.k8s_service.helm_uninstall(release_name=install)
        for configmap in self.k8s_service.get_configmaps():
            if pattern.match(configmap) and configmap.endswith("migration-config"):
                self.k8s_service.delete_configmap(configmap_name=configmap)

        # Cleanup non-Helm Kubernetes resources (ES 1.x and 2.x)
        try:
            self.k8s_service.run_command([
                "kubectl", "delete", "all,configmap,secret",
                "-l", "migration-test=true",
                "--ignore-not-found"
            ], ignore_errors=True)
        except Exception as e:
            logger.warning(f"Failed to cleanup labeled Kubernetes resources: {e}")

    def cleanup_deployment(self) -> None:
        self.cleanup_clusters()
        self.k8s_service.helm_uninstall(release_name=MA_RELEASE_NAME)
        self.k8s_service.wait_for_all_healthy_pods()
        self.k8s_service.delete_all_pvcs()

    def copy_logs(self, destination: str = "./logs") -> None:
        self.k8s_service.copy_log_files(destination=destination)

    def run(self, skip_delete: bool = False, keep_workflows: bool = False, developer_mode: bool = False,
            reuse_clusters: bool = False, test_reports_dir: str = None, copy_logs: bool = False) -> None:
        self.k8s_service.create_namespace(self.k8s_service.namespace)
        if developer_mode:
            workflow_templates_dir = (
                "../../TrafficCapture/dockerSolution/src/main/docker/migrationConsole/"
                "workflows/templates/"
            )
            self.k8s_service.run_command([
                "kubectl", "apply", "-f", workflow_templates_dir, "-n", "ma"
            ])
            logger.info("Applied local workflow templates directory")

        combos_with_failures = []
        test_reports = []
        last_combination = self.combinations[-1]
        for source_version, target_version in self.combinations:
            is_last = (source_version, target_version) == last_combination
            try:
                logger.info(f"Performing helm deployment for migration testing environment "
                            f"from {source_version} to {target_version}")

                chart_values = {"developerModeEnabled": "true"} if developer_mode else None
                if not self.k8s_service.helm_install(chart_path=self.ma_chart_path, release_name=MA_RELEASE_NAME,
                                                     values=chart_values):
                    raise HelmCommandFailed("Helm install of Migrations Assistant chart failed")

                self.k8s_service.wait_for_all_healthy_pods()

                test_report = self.run_tests(source_version=source_version,
                                             target_version=target_version,
                                             keep_workflows=keep_workflows,
                                             reuse_clusters=reuse_clusters,
                                             test_reports_dir=test_reports_dir)
                test_reports.append(test_report)
                tests_failed = test_report.summary.passed == 0 or test_report.summary.failed > 0

                if tests_failed:
                    logger.warning(f"Tests failed (or no tests executed) for migrations "
                                   f"from {source_version} to {target_version}.")
                    combos_with_failures.append(f"{source_version} -> {target_version}")
                else:
                    logger.info(f"Tests passed successfully for migrations "
                                f"from {source_version} to {target_version}.")
            except HelmCommandFailed as helmError:
                logger.error(f"Helm command failed with error: {helmError}. Testing may be incomplete")
            except TimeoutError as timeoutError:
                logger.error(f"Timeout error encountered: {timeoutError}. Testing may be incomplete")

            # We need to copy logs once and before the migration console is uninstalled
            if is_last and copy_logs:
                self.copy_logs()

            if not skip_delete:
                self.cleanup_deployment()

        self._print_summary_table(reports=test_reports)
        if combos_with_failures:
            raise TestsFailed(f"The following combinations had test failures (or no test cases executed): "
                              f"{combos_with_failures}")
        logger.info("Test execution completed.")


def _parse_test_ids(test_ids_str: str) -> List[str]:
    # Split the string by commas and remove extra whitespace
    return [tid.strip() for tid in test_ids_str.split(",") if tid.strip()]


def _generate_unique_id() -> str:
    """Generate a human-readable unique ID with a timestamp and a 4-character random string."""
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    random_part = ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))
    return f"{random_part}-{timestamp}"


def get_version_combinations(source_version, target_version):
    source_list = VALID_SOURCE_VERSIONS if source_version == "all" else [source_version]
    target_list = VALID_TARGET_VERSIONS if target_version == "all" else [target_version]

    # Cartesian product of source and target lists
    combos = [(s, t) for s in source_list for t in target_list]
    return combos


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Process inputs for test automation runner"
    )
    source_versions = VALID_SOURCE_VERSIONS + ['all']
    target_versions = VALID_TARGET_VERSIONS + ['all']
    parser.add_argument(
        "--source-version",
        choices=source_versions,
        default="ES_5.6",
        help=f"Source version to use. Must be one of: {', '.join(source_versions)}"
    )
    parser.add_argument(
        "--target-version",
        choices=target_versions,
        default="OS_2.19",
        help=f"Target version to use. Must be one of: {', '.join(target_versions)}"
    )
    parser.add_argument(
        "--skip-delete",
        action="store_true",
        help="If set, skip deletion operations."
    )
    parser.add_argument(
        "--delete-only",
        action="store_true",
        help="If set, only perform deletion operations."
    )
    parser.add_argument(
        "--delete-clusters-only",
        action="store_true",
        help="If set, only perform cluster deletion operations."
    )
    parser.add_argument(
        "--copy-logs",
        action="store_true",
        help="If set, will copy found argo workflow logs to the current directory."
    )
    parser.add_argument(
        '--unique-id',
        type=str,
        default=_generate_unique_id(),
        help="Provide a unique ID for labeling test resources, or generate one by default"
    )
    parser.add_argument(
        "--keep-workflows",
        action="store_true",
        help="If set, will not delete argo workflows created by integration tests"
    )
    parser.add_argument(
        "--developer-mode",
        action="store_true",
        help="If set, will enable the developer mode flag for the Migration Assistant helm chart"
    )
    parser.add_argument(
        "--reuse-clusters",
        action="store_true",
        help="If set, the integration tests will reuse existing clusters that match the naming pattern "
             "e.g. 'target-opensearch-2-19-*'. If a cluster does not exist the integration test will create it and "
             "leave it running once the test completes for other tests to use. The cleanup operation for this library "
             "will remove all source and target clusters that match this pattern as well."
    )
    parser.add_argument(
        "--dev",
        action="store_true",
        help="An aggregate flag to apply developer settings for "
             "testing [--skip-delete, --reuse-clusters, --keep-workflows, --developer-mode]"
    )
    parser.add_argument(
        "--test-reports-dir",
        default=None,
        help="If provided, will output generated test reports to this directory path"
    )
    parser.add_argument(
        "--output-reports-summary-only",
        action="store_true",
        help="If set, only print the summary table for existing reports in the provided '--test-reports-dir'"
    )
    parser.add_argument(
        "--test-ids",
        type=_parse_test_ids,
        default=[],
        help="Comma-separated list of test IDs to run (e.g. 0001,0003)"
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    k8s_service = K8sService()
    helm_k8s_base_path = "../../deployment/k8s"
    helm_charts_base_path = f"{helm_k8s_base_path}/charts"
    ma_chart_path = f"{helm_charts_base_path}/aggregates/migrationAssistantWithArgo"

    combinations = get_version_combinations(source_version=args.source_version, target_version=args.target_version)
    logger.info("Detected the following version combinations to test:\n" +
                "\n".join([f"- {src} → {tgt}" for src, tgt in combinations]))
    test_runner = TestRunner(k8s_service=k8s_service,
                             unique_id=args.unique_id,
                             test_ids=args.test_ids,
                             ma_chart_path=ma_chart_path,
                             combinations=combinations)

    if args.delete_only:
        return test_runner.cleanup_deployment()
    if args.delete_clusters_only:
        return test_runner.cleanup_clusters()
    if args.output_reports_summary_only:
        if not args.test_reports_dir:
            raise ValueError("The '--test-reports-dir' arg must be provided when using '--output-reports-summary-only")
        return test_runner.collect_reports_and_print_summary(reports_dir=args.test_reports_dir)
    skip_delete = args.skip_delete
    keep_workflows = args.keep_workflows
    developer_mode = args.developer_mode
    reuse_clusters = args.reuse_clusters
    if args.dev:
        skip_delete = True
        keep_workflows = True
        developer_mode = True
        reuse_clusters = True
    if len(combinations) > 1 and (skip_delete or reuse_clusters):
        logger.warning("Disabling the --skip-delete and --reuse-clusters options, as they cannot be used with more "
                       "than one version combination")
        skip_delete = False
        reuse_clusters = False
    test_runner.run(skip_delete=skip_delete,
                    keep_workflows=keep_workflows,
                    developer_mode=developer_mode,
                    reuse_clusters=reuse_clusters,
                    test_reports_dir=args.test_reports_dir,
                    copy_logs=args.copy_logs)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        # Handle Ctrl+C cleanly too
        sys.exit(0)
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)
