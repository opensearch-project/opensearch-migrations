import argparse
import ast
from dataclasses import dataclass, field
import datetime
from k8s_service import K8sService, HelmCommandFailed
import logging
import random
import string
import sys
from tabulate import tabulate
from typing import List, Optional, Tuple

logging.basicConfig(format='%(asctime)s [%(levelname)s] %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

VALID_SOURCE_VERSIONS = ["ES_5.6"]
VALID_TARGET_VERSIONS = ["OS_2.19"]
MA_RELEASE_NAME = "ma"


# Data classes to represent test output generated by python e2e tests
@dataclass
class TestEntry:
    name: str
    description: str
    result: str
    duration: float
    error: Optional[str] = None


@dataclass
class TestSummary:
    passed: int
    failed: int
    source_version: str
    target_version: str


@dataclass
class TestReport:
    summary: TestSummary
    tests: List[TestEntry] = field(default_factory=list)


class TestsFailed(Exception):
    pass


class TestRunner:

    def __init__(self, k8s_service: K8sService, unique_id: str, test_ids: List[str], ma_chart_path: str,
                 combinations: List[Tuple[str, str]]) -> None:
        self.k8s_service = k8s_service
        self.unique_id = unique_id
        self.test_ids = test_ids
        self.ma_chart_path = ma_chart_path
        self.combinations = combinations

    def _print_test_stats(self, report: TestReport) -> None:
        for test in report.tests:
            print(f"{test.name}:")
            print(f"  - result: {test.result}")
            print(f"  - duration: {test.duration:.5f} seconds")
            if test.error:
                print(f"  - error: {test.error}")
            print()

    def _print_summary_table(self, reports: List[TestReport]) -> None:
        all_test_names = sorted({test.name for report in reports for test in report.tests})

        # Build the test matrix rows
        matrix_rows = []
        for report in reports:
            version_label = f"{report.summary.source_version} -> {report.summary.target_version}"
            row = [version_label]
            test_results = {test.name: "âœ“" if test.result == "passed" else "X" for test in report.tests}
            for name in all_test_names:
                row.append(test_results.get(name, ""))
            matrix_rows.append(row)

        # Build test description rows
        test_descriptions = {}
        for report in reports:
            for test in report.tests:
                test_descriptions.setdefault(test.name, test.description)

        # Print Test Matrix
        headers = ["Version"] + all_test_names
        print("\nTest Matrix:")
        print(tabulate(matrix_rows, headers=headers, tablefmt="fancy_grid"))

        # Print Test Case Information
        description_table = [[name, test_descriptions[name]] for name in all_test_names]
        print("\nTest Case Information:")
        print(tabulate(description_table, headers=["Test Name", "Description"], tablefmt="fancy_grid"))

        # Print Test Stats
        print("\nTest Stats:")
        for report in reports:
            print(f"===== {report.summary.source_version} -> {report.summary.target_version} =====")
            self._print_test_stats(report)

    def _parse_test_report(self, data: dict) -> TestReport:
        tests = [TestEntry(**test) for test in data.get("tests", [])]
        summary = TestSummary(**data.get("summary"))
        return TestReport(tests=tests, summary=summary)

    def run_tests(self, source_version: str, target_version: str, keep_workflows: bool = False) -> bool:
        """Runs pytest tests."""
        logger.info(f"Executing migration test cases with pytest and test ID filters: {self.test_ids}")
        command_list = [
            "pipenv",
            "run",
            "pytest",
            "/root/lib/integ_test/integ_test/ma_workflow_test.py",
            f"--unique_id={self.unique_id}",
            f"--source_version={source_version}",
            f"--target_version={target_version}"
        ]
        if self.test_ids:
            command_list.append(f"--test_ids={','.join(self.test_ids)}")
        if keep_workflows:
            command_list.append("--keep_workflows")
        command_list.append("-s")
        self.k8s_service.exec_migration_console_cmd(command_list=command_list)
        output_file_path = f"/root/lib/integ_test/results/{self.unique_id}/test_report.json"
        logger.info(f"Retrieving test report at {output_file_path}")
        cmd_response = self.k8s_service.exec_migration_console_cmd(command_list=["cat", output_file_path],
                                                                   unbuffered=False)
        test_data = ast.literal_eval(cmd_response)
        logger.debug(f"Received the following test data: {test_data}")
        test_report = self._parse_test_report(test_data)
        print(f"Test cases passed: {test_report.summary.passed}")
        print(f"Test cases failed: {test_report.summary.failed}")
        self._print_summary_table(reports=[test_report])
        if test_report.summary.passed == 0 or test_report.summary.failed > 0:
            return False
        return True

    def cleanup_deployment(self) -> None:
        self.k8s_service.helm_uninstall(release_name=MA_RELEASE_NAME)
        self.k8s_service.wait_for_all_healthy_pods()
        self.k8s_service.delete_all_pvcs()

    def copy_logs(self, destination: str = "./logs") -> None:
        self.k8s_service.copy_log_files(destination=destination)

    def run(self, skip_delete: bool = False, keep_workflows: bool = False) -> None:
        for source_version, target_version in self.combinations:
            try:
                logger.info(f"Performing helm deployment for migration testing environment "
                            f"from {source_version} to {target_version}")

                if not self.k8s_service.helm_install(chart_path=self.ma_chart_path, release_name=MA_RELEASE_NAME):
                    raise HelmCommandFailed("Helm install of Migrations Assistant chart failed")

                self.k8s_service.wait_for_all_healthy_pods()

                tests_passed = self.run_tests(source_version=source_version,
                                              target_version=target_version,
                                              keep_workflows=keep_workflows)

                if not tests_passed:
                    raise TestsFailed(f"Tests failed (or no tests executed) for migrations "
                                      f"from {source_version} to {target_version}.")
                else:
                    logger.info(f"Tests passed successfully for migrations "
                                f"from {source_version} to {target_version}.")
            except HelmCommandFailed as helmError:
                logger.error(f"Helm command failed with error: {helmError}. Testing may be incomplete")
            except TimeoutError as timeoutError:
                logger.error(f"Timeout error encountered: {timeoutError}. Testing may be incomplete")

            if not skip_delete:
                self.cleanup_deployment()

        logger.info("Test execution completed.")


def _parse_test_ids(test_ids_str: str) -> List[str]:
    # Split the string by commas and remove extra whitespace
    return [tid.strip() for tid in test_ids_str.split(",") if tid.strip()]


def _generate_unique_id() -> str:
    """Generate a human-readable unique ID with a timestamp and a 4-character random string."""
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    random_part = ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))
    return f"{random_part}-{timestamp}"


def parse_version_string(version_str: str) -> (string, string, string):
    """Parse a string in format ES|OS_x.y and return the distinct pieces as (cluster_type, major, minor)"""
    try:
        cluster_type_part, version_part = version_str.split('_', 1)
        major_str, minor_str = version_part.split('.', 1)

        cluster_type = cluster_type_part.lower()
        major = int(major_str)

        try:
            minor = int(minor_str)
        except ValueError:
            minor = minor_str

        return cluster_type, major, minor
    except (ValueError, AttributeError):
        raise ValueError(f"Invalid version string format: '{version_str}'")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Process inputs for test automation runner"
    )
    parser.add_argument(
        "--source-version",
        choices=VALID_SOURCE_VERSIONS,
        default="ES_5.6",
        help=f"Source version to use. Must be one of: {', '.join(VALID_SOURCE_VERSIONS)}"
    )
    parser.add_argument(
        "--target-version",
        choices=VALID_TARGET_VERSIONS,
        default="OS_2.19",
        help=f"Target version to use. Must be one of: {', '.join(VALID_TARGET_VERSIONS)}"
    )
    parser.add_argument(
        "--skip-delete",
        action="store_true",
        help="If set, skip deletion operations."
    )
    parser.add_argument(
        "--delete-only",
        action="store_true",
        help="If set, only perform deletion operations."
    )
    parser.add_argument(
        "--copy-logs-only",
        action="store_true",
        help="If set, only copy found argo workflow logs to this local directory."
    )
    parser.add_argument(
        '--unique-id',
        type=str,
        default=_generate_unique_id(),
        help="Provide a unique ID for labeling test resources, or generate one by default"
    )
    parser.add_argument(
        "--keep-workflows",
        action="store_true",
        help="If set, will not delete argo workflows created by integration tests"
    )
    parser.add_argument(
        "--test-ids",
        type=_parse_test_ids,
        default=[],
        help="Comma-separated list of test IDs to run (e.g. 0001,0003)"
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    k8s_service = K8sService()
    helm_k8s_base_path = "../../deployment/k8s"
    helm_charts_base_path = f"{helm_k8s_base_path}/charts"
    ma_chart_path = f"{helm_charts_base_path}/aggregates/migrationAssistantWithArgo"

    combinations = [(args.source_version, args.target_version)]
    test_runner = TestRunner(k8s_service=k8s_service,
                             unique_id=args.unique_id,
                             test_ids=args.test_ids,
                             ma_chart_path=ma_chart_path,
                             combinations=combinations)

    if args.delete_only:
        return test_runner.cleanup_deployment()
    elif args.copy_logs_only:
        return test_runner.copy_logs()
    test_runner.run(skip_delete=args.skip_delete, keep_workflows=args.keep_workflows)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        # Handle Ctrl+C cleanly too
        sys.exit(0)
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)
