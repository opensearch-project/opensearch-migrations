conditionalPackageInstalls:
  kyverno: false
  # Migration infrastructure components
  cert-manager: true            # needed by otel operator (which is off by default)
  kube-prometheus-stack: true   # will be needed for auto-scaling
  argo-workflows: true          # orchestrates the steps of a migration
  strimzi-kafka-operator: true  # Required by capture and replay
  fluent-bit: true

  # Simpler alternative to the opentelemetry-operator.
  # ONLY set up the otel-collector and require manual instrumentation
  # Mutually exclusive with otel-collector
  otel-collector-daemonset: true
  # EXPERIMENTAL!  Needed for auto-instrumenting and general observability.
  # Mutually exclusive with otel-collector-daemonset
  opentelemetry-operator: false

  # Support packages - installed as direct dependencies to this chart as opposed to via the installer job
  migration-console: true

  # Packages for testing and local development
  localstack: true
  # Nice to haves that aren't fully supported yet
  gatekeeper: false
  grafana: false
  jaeger: true

# Kyverno policy toggles (requires conditionalPackageInstalls.kyverno: true)
kyvernoPolicies:
  zeroResourceRequests: false
  mountLocalAwsCreds: false
  mountLocalAwsCredsPath: "~/.aws"

images:
  captureProxy:
    repository: migrations/capture_proxy
    tag: latest
    pullPolicy: IfNotPresent
  trafficReplayer:
    repository: migrations/traffic_replayer
    tag: latest
    pullPolicy: IfNotPresent
  reindexFromSnapshot:
    repository: migrations/reindex_from_snapshot
    tag: latest
    pullPolicy: IfNotPresent
  migrationConsole:
    repository: migrations/migration_console
    tag: latest
    pullPolicy: IfNotPresent
  installer:
    repository: migrations/migration_console
    tag: latest
    pullPolicy: IfNotPresent

# Default AWS values are dummy values for LocalStack usage
aws:
  configureAwsEksResources: false
  region: us-east-2
  account: "123456789012"

# EKS cluster configuration for Karpenter NodeClass
cluster:
  # Set to true for EKS clusters with Karpenter, false for other cluster types
  dedicatedKarpenterNodePoolForMigrationConsole: false
  useCustomKarpenterNodePool: false

# Migrations NodePool configuration (EKS only)
workloadsNodePool:
  architectures: []  # Empty = no constraint (Karpenter picks). Set ["amd64"] or ["arm64"] to restrict.
  # These limits are keep the entire aggregate compute size under these values.
  # This is an arbiter on total cost that a deployment can consume per unit of time
  limits:
    cpu: "64000m"
    memory: "128Gi"

stageName: dev

defaultBucketConfiguration:
  create: true
  deleteOnUninstall: true
  emptyBeforeDelete: true
  useLocalStack: true
  endpoint: localstack
  serviceAccountName: "migrations-service-account"
  bucketOperationImage: "amazon/aws-cli:2.25.11"
  bucketOperationImagePullPolicy: "IfNotPresent"

logs:
  sharedLogsVolume:
    enabled: true
    name: logs-pvc
    size: 10Gi
    storageClass: ""
  format: application # application or docker-json


installer:
  serviceAccount:
    create: true
    name: "migrations-service-account"
  rbac:
    create: true

# For OTEL Operator
extraOtelConfiguration:
  version: "0.124.0"
  configs:
    metrics:
      - prometheus
      - cloudwatch
    traces:
      - jaeger
      - xray

# For OTEL Daemonset
otelConfiguration:
  serviceAccountName: "otel-collector"
  collectorConfig: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
      prometheus:
        config:
          scrape_configs:
            - job_name: 'kubelet-cadvisor'
              scrape_interval: 5s
              scheme: https
              authorization: # Use the ServiceAccount token for authorization
                credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                insecure_skip_verify: true # Minikube uses self-signed certs
              kubernetes_sd_configs:
                - role: node
              relabel_configs:
                # Only scrape the local node (important for DaemonSet)
                - source_labels: [__meta_kubernetes_node_name]
                  action: keep
                  regex: ${K8S_NODE_NAME}
                # Change path from /metrics to /metrics/cadvisor
                - target_label: __metrics_path__
                  replacement: /metrics/cadvisor
                # Keep node name as a label
                - source_labels: [__meta_kubernetes_node_name]
                  target_label: node
    processors:
      batch:
        timeout: 10s
        send_batch_size: 8192
        send_batch_max_size: 10000
    extensions:
      zpages:
        endpoint: :55679
      pprof:
        endpoint: :1888
      health_check:
    exporters:
      debug:
        verbosity: detailed
        sampling_initial: 5
        sampling_thereafter: 200
      prometheus:
        endpoint: "0.0.0.0:8889"
        send_timestamps: true
        metric_expiration: 5m
        enable_open_metrics: true
      otlp/jaeger: # Jaeger supports OTLP directly. The default port for OTLP/gRPC is 4317
        endpoint: jaeger-collector:4317
        tls:
          insecure: true
    service:
      telemetry:
        logs:
          level: warn
      extensions: [ zpages, pprof, health_check ]
      pipelines:
        metrics:
          receivers: [ otlp, prometheus ]
          processors: [ batch ]
          exporters: [ prometheus ]
        traces:
          receivers: [ otlp ]
          processors: [ batch ]
          exporters: [ otlp/jaeger ]
        logs:
          receivers: [ otlp ]
          processors:
          exporters: [ debug ]

charts:
  cert-manager:
    version: 1.17.2
    repository: https://charts.jetstack.io
    waitForInstallation: true
    values:
      crds:
        enabled: true

  opentelemetry-operator:
    version: "0.86.4"
    repository: "https://open-telemetry.github.io/opentelemetry-helm-charts"
    timeout: 300
    dependsOn: cert-manager
    values:
      admissionWebhooks:
        certManager:
          enabled: true # default
      crds:
        create: true
      manager:
        verticalPodAutoscaler:
          enabled: false # default
        collectorImage:
          repository: "public.ecr.aws/aws-observability/aws-otel-collector"
          tag: "0.43.2"


  strimzi-kafka-operator:
    version: 0.50.0
    repository: "https://strimzi.io/charts/"
    values:
      storageType: ephemeral # strimzi is stateless
      storageSize: 5Gi

      # Increase probe thresholds to prevent unnecessary restarts during high API load.
      readinessProbe:
        initialDelaySeconds: 15
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3
      livenessProbe:
        initialDelaySeconds: 15
        periodSeconds: 15
        timeoutSeconds: 5
        failureThreshold: 3


  argo-workflows:
    version: "0.47.1"
    repository: "https://argoproj.github.io/argo-helm"
    values:
      fullnameOverride: "argo"
      images:
        # Override appVersion v3.7.8 to v3.7.9 to incorporate retry delay overflow fix
        # https://github.com/argoproj/argo-workflows/pull/15277
        tag: v3.7.9
        pullPolicy: IfNotPresent
      controller:
        pdb:
          enabled: true
          minAvailable: 1  # Ensures at least 1 pod always running
        containerSecurityContext:
          runAsNonRoot: true
          allowPrivilegeEscalation: false
        clusterWorkflowTemplates:
          enabled: false
        metricsConfig:
          enabled: true
        resourceRateLimit: { "limit": 10.0, "burst": 25 }
        workflowWorkers: 16

        replicas: 2
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchLabels:
                      app.kubernetes.io/name: argo-workflows-controller
                  topologyKey: kubernetes.io/hostname
      mainContainer:
        env:
          - name: RESOURCE_STATE_CHECK_INTERVAL
            value: "1s"
      executor:
        env:
          - name: RESOURCE_STATE_CHECK_INTERVAL
            value: "1s"
      server:
        replicas: 2
        pdb:
          enabled: true
          minAvailable: 1  # Ensures at least 1 pod always running
        livenessProbe:
          enabled: true
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchLabels:
                      app.kubernetes.io/name: argo-workflows-server
                  topologyKey: kubernetes.io/hostname
        extraArgs:
          - --auth-mode=server
      serviceAccount:
        create: true
      singleNamespace: true
      # Extra workflow configuration
      workflow:
        serviceAccount:
          create: false
          name: argo-workflow-executor

  fluent-bit:
    version: "0.49.0"
    repository: "https://fluent.github.io/helm-charts"
    values:
      env:
        - name: OUTPUT_FORMAT
          valueFrom:
            configMapKeyRef:
              name: log-aggregation-config
              key: OUTPUT_FORMAT

      extraVolumes:
        - name: logs-pv
          persistentVolumeClaim:
            claimName: logs-pvc
        - name: lua-scripts
          configMap:
            name: fluentbit-lua-scripts

      extraVolumeMounts:
        - name: logs-pv
          mountPath: /shared_logs
        - name: lua-scripts
          mountPath: /fluentbit/scripts
          readOnly: true

      config:
        customParsers: |
          [PARSER]
              Name               logfmt_parser
              Format             logfmt
          
          [PARSER]
              Name               java_log_parser
              Format             regex
              Regex              ^(?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) (?<level>[A-Z]+)\s+(?<source>[\w\d]+:\d+) - (?<msg>.*)$
              Time_Key           time
              Time_Format        %Y-%m-%d %H:%M:%S
    

        inputs: |
          [INPUT]
              Name               tail
              Tag                kube.*
              Path               /var/log/containers/*.log
              multiline.parser   docker, cri
              Refresh_Interval   5
              Mem_Buf_Limit      5MB
              Skip_Long_Lines    Off
              Skip_Empty_Lines   On

        filters: |
          [FILTER]
              Name               kubernetes
              Match              kube.*
              Kube_URL           https://kubernetes.default.svc:443
              Kube_CA_File       /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              Kube_Token_File    /var/run/secrets/kubernetes.io/serviceaccount/token
              Kube_Tag_Prefix    kube.var.log.containers.
              Merge_Log          On
              Merge_Log_Key      log_processed
              Annotations        Off
          
          [FILTER]
              Name               rewrite_tag
              Match              kube.*
              Rule               $kubernetes['pod_name'] ^(.*)$ fluentbit-$1 false
              Emitter_Name       re_emitted
          
          [FILTER]
              Name               parser
              Match_Regex        ^fluentbit-argo-.*$|^fluentbit-.*-(run-console|create-rs|delete-rs)-.*$
              Key_Name           log
              Parser             logfmt_parser
              Preserve_Key       On
              Reserve_Data       On
      
          [FILTER]
              Name               parser
              Match              fluentbit-*-reindex-from-snapshot-*
              Key_Name           log
              Parser             java_log_parser
              Preserve_Key       On
              Reserve_Data       On
          
          [FILTER]
              Name               lua
              Match              fluentbit-*
              script             /fluentbit/scripts/trim.lua
              call               trim_log
    
          [FILTER]
              Name               nest
              Match              fluentbit-*
              Operation          lift
              Nested_under       kubernetes

          [FILTER]
              Name               nest
              Match              fluentbit-*
              Operation          lift
              Nested_under       labels
          
          [FILTER]
              Name               modify
              Match              fluentbit-*
              Rename             container_name container
              Rename             container_image image
              Rename             pod_name pod
              Rename             namespace_name namespace
              Rename             workflows.argoproj.io/workflow workflow
              Remove             _p
              Remove             docker_id
              Remove             container_hash
              Remove             pod-template-hash
              Remove             workflows.argoproj.io/completed
              Remove_regex       ^app\.kubernetes\.io
              Remove_regex       ^batch\.kubernetes\.io
              Move_to_start      log

        outputs: |
          [OUTPUT]
              Name               file
              Match              fluentbit-*
              Path               /shared_logs
              Format             ${OUTPUT_FORMAT}
              Template           {log}

      serviceAccount:
        create: false
        name: migrations-service-account

  # Will likely be required for auto-scaling.
  # Notice that we don't install otel-collector yet, so we can't take advantage of this anyway.
  kube-prometheus-stack:
    version: "72.0.0"
    repository: "https://prometheus-community.github.io/helm-charts"
    values:
      prometheus:
        prometheusSpec:
          serviceMonitorSelector: { }  # Select all ServiceMonitors
          podMonitorSelector: { }      # Select all PodMonitors
          serviceMonitorNamespaceSelector: { }
          podMonitorNamespaceSelector: { }
          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "1000m"
          retention: "15d"
          retentionSize: "10GB"
          storageSpec:
            volumeClaimTemplate:
              spec:
                accessModes: [ "ReadWriteOnce" ]
                resources:
                  requests:
                    storage: 50Gi
          thanos:
            enabled: false
        service:
          type: ClusterIP
          port: 9090
      alertmanager:
        enabled: false
      nodeExporter:
        enabled: true  # If OpenTelemetry for is handling host metrics, this can be disabled
      kubeStateMetrics:
        enabled: true
      defaultRules:
        create: false
        rules:
          alertmanager: true
          etcd: true
          general: true
          k8s: true
          kubeApiserver: true
          kubePrometheusNodeAlerting: true
          kubePrometheusNodeRecording: true
          kubernetesAbsent: true
          kubernetesApps: true
          kubernetesResources: true
          kubernetesStorage: true
          kubernetesSystem: true
          kubeScheduler: true
          network: true
          node: true
          prometheus: true
          prometheusOperator: true
      grafana:
        additionalDataSources:
          - name: Jaeger
            type: jaeger
            url: http://jaeger-query:16686
            access: proxy
            isDefault: false
            editable: true

# OPA Gatekeeper dependency to lock down what images we can spin up - not implemented yet
#  - name: gatekeeper
#    version: "3.13.0"
#    repository: "https://open-policy-agent.github.io/gatekeeper/charts"
#    values:
#      auditInterval: 60
#      constraintViolationsLimit: 20
#      enableExternalData: true

  localstack:
    version: "0.6.23"  # Or whichever version you want to use
    repository: "https://localstack.github.io/helm-charts"
    values:
      image:
        repository: "localstack/localstack"
        tag: "4.3.0"

  grafana:
    version: "8.15.0"
    repository: "https://grafana.github.io/helm-charts"
    values:
      ## Grafana data sources configuration
      datasources:
        datasources.yaml:
          apiVersion: 1
          datasources:
            - name: Prometheus
              type: prometheus
              access: proxy
              url: http://prometheus-server.prometheus.svc.cluster.local:9090
              isDefault: true
              editable: true
            - name: Jaeger
              type: jaeger
              access: proxy
              url: http://jaeger-query.jaeger.svc.cluster.local:16686
              isDefault: false
              editable: true
#      ## Set up the sidecar to import the dashboard yaml included in this package
#      sidecar:
#        datasources:
#          enabled: true
#        dashboards:
#          enabled: true
#          label: grafana_dashboard

  jaeger:
    version: "3.2.0"
    repository: "https://jaegertracing.github.io/helm-charts"
    values:
      provisionDataStore:
        cassandra: false
      allInOne:
        enabled: true
      storage:
        type: memory
      agent:
        enabled: false
      collector:
        enabled: false
      query:
        enabled: false

  kyverno:
    version: "3.5.2"
    repository: "https://kyverno.github.io/kyverno/"
    namespace: ma
    values:
      admissionController:
        replicas: 1
        container:
          resources:
            requests: null
            limits: null
        initContainer:
          resources:
            requests: null
            limits: null
      backgroundController:
        replicas: 1
        resources:
          requests: null
          limits: null
      cleanupController:
        replicas: 1
        resources:
          requests: null
          limits: null
      reportsController:
        replicas: 1
        resources:
          requests: null
          limits: null
      config:
        excludeKyvernoNamespace: false
