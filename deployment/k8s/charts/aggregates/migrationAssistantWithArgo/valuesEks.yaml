conditionalPackageInstalls:
  localstack: false
  jaeger: false

aws:
  configureAwsEksResources: true
  region: ""
  account: ""

cluster:
  dedicatedKarpenterNodePoolForMigrationConsole: true
  # set this to true and disable the general-purpose pool to put a hard limit resource usage
  useCustomKarpenterNodePool: false

# Enable multi-arch for EKS
workloadsNodePool:
  architectures: ["amd64", "arm64"]

defaultBucketConfiguration:
  useLocalStack: false
  deleteOnUninstall: true
  emptyBeforeDelete: true
  endpoint: ""
  snapshotRoleArn: ""

# Useful for updating images while developing
#images:
#  captureProxy:
#    pullPolicy: Always
#  trafficReplayer:
#    pullPolicy: Always
#  reindexFromSnapshot:
#    pullPolicy: Always
#  migrationConsole:
#    pullPolicy: Always
#  installer:
#    pullPolicy: Always

# For OTEL Daemonset
otelConfiguration:
  env:
    - name: AWS_REGION
      valueFrom:
        configMapKeyRef:
          name: aws-metadata
          key: AWS_REGION
  collectorConfig: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
      prometheus:
        config:
          scrape_configs:
            - job_name: 'cadvisor'
              scrape_interval: 5s
              scheme: https
              authorization:
                credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              kubernetes_sd_configs:
                - role: node
              relabel_configs:
                # Only scrape the local node (important for DaemonSet)
                - source_labels: [__meta_kubernetes_node_name]
                  action: keep
                  regex: ${K8S_NODE_NAME}
                # Change path from /metrics to /metrics/cadvisor
                - target_label: __metrics_path__
                  replacement: /metrics/cadvisor
                # Keep node name as a label
                - source_labels: [__meta_kubernetes_node_name]
                  target_label: node
    processors:
      batch:
        timeout: 10s
        send_batch_size: 8192
        send_batch_max_size: 10000
      cumulativetodelta:
      resource/metrics:
        attributes:
          - key: qualifier
            value: ${env:QUALIFIER}
            action: upsert
      resource/remove_default_attributes:
        attributes:
          - key: telemetry.sdk.name
            action: delete
          - key: telemetry.sdk.version
            action: delete
          - key: telemetry.sdk.language
            action: delete
          - key: service.name
            action: delete
    extensions:
      health_check:
        endpoint: :13133
    exporters:
      debug:
        verbosity: detailed
        sampling_initial: 5
        sampling_thereafter: 200
      awsemf:
        namespace: 'OpenSearchMigrations'
        dimension_rollup_option: NoDimensionRollup # Reduce number of metrics by only publishing with all dimensions
        resource_to_telemetry_conversion:
          enabled: true
      prometheus:
        endpoint: "0.0.0.0:8889"
        send_timestamps: true
        metric_expiration: 5m
        enable_open_metrics: true
    service:
      extensions: [health_check]
      pipelines:
        metrics:
          receivers: [otlp, prometheus]
          processors: [batch, resource/remove_default_attributes, resource/metrics, cumulativetodelta]
          exporters: [awsemf, prometheus]

charts:
  argo-workflows:
    values:
      controller:
        replicas: 2
        pdb:
          enabled: true
          minAvailable: 1
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchLabels:
                      app.kubernetes.io/name: argo-workflows-controller
                  topologyKey: kubernetes.io/hostname
        persistence:
          postgresql:
            ssl: true
            sslMode: require
      server:
        replicas: 2
        pdb:
          enabled: true
          minAvailable: 1
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchLabels:
                      app.kubernetes.io/name: argo-workflows-server
                  topologyKey: kubernetes.io/hostname

  fluent-bit:
    values:
      env:
        - name: AWS_REGION
          valueFrom:
            configMapKeyRef:
              name: aws-metadata
              key: AWS_REGION
        - name: STAGE_NAME
          valueFrom:
            configMapKeyRef:
              name: aws-metadata
              key: STAGE_NAME

      config:
        # Change output destination to be CloudWatch
        outputs: |
          [OUTPUT]
              Name                cloudwatch_logs
              Match               fluentbit-*
              region              ${AWS_REGION}
              log_group_name      /migration-assistant-${STAGE_NAME}-${AWS_REGION}/logs
              log_stream_prefix   from-
              auto_create_group   true

      # Remove logs PVC mount for EKS deployment
      extraVolumes:
        - name: lua-scripts
          configMap:
            name: fluentbit-lua-scripts

      extraVolumeMounts:
        - name: lua-scripts
          mountPath: /fluentbit/scripts
          readOnly: true


# Higher resources for EKS production
etcdCluster:
  cpu: 500m
  memory: 1Gi

# HA CNPG cluster for EKS production
cnpgCluster:
  instances: 2
  storageSize: "10Gi"
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"
  # Prevent both instances from being disrupted simultaneously
  podAntiAffinityType: "required"        # Hard anti-affinity: instances MUST be on different nodes
  topologyKey: "topology.kubernetes.io/zone"  # Spread across AZs for zone failure resilience
